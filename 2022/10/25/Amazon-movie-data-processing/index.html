<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Amazon电影数据处理 | Gxyrious's Blog</title><meta name="keywords" content="数据分析"><meta name="author" content="Gxyrious,2466445001@qq.com"><meta name="copyright" content="Gxyrious"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="序言最近学校有一门课，需要爬取Amazon的电影数据并作清洗和整理，这里简要记录一些思路和困难。 数据爬取 准备工作 数据来源是一个包含了7911684个用户评价的txt文档，这里是下载链接。下载完解压后是一个9G左右的movies.txt文档，下面是一条用户评价的示例：  product&#x2F;productId: B00006HAXW review&#x2F;userId: A1RSDE9">
<meta property="og:type" content="article">
<meta property="og:title" content="Amazon电影数据处理">
<meta property="og:url" content="http://gxyrious.github.io/2022/10/25/Amazon-movie-data-processing/index.html">
<meta property="og:site_name" content="Gxyrious&#39;s Blog">
<meta property="og:description" content="序言最近学校有一门课，需要爬取Amazon的电影数据并作清洗和整理，这里简要记录一些思路和困难。 数据爬取 准备工作 数据来源是一个包含了7911684个用户评价的txt文档，这里是下载链接。下载完解压后是一个9G左右的movies.txt文档，下面是一条用户评价的示例：  product&#x2F;productId: B00006HAXW review&#x2F;userId: A1RSDE9">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-nz8x1v_1920x1080.png">
<meta property="article:published_time" content="2022-10-25T04:33:39.000Z">
<meta property="article:modified_time" content="2023-01-21T05:33:08.653Z">
<meta property="article:author" content="Gxyrious">
<meta property="article:tag" content="数据分析">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-nz8x1v_1920x1080.png"><link rel="shortcut icon" href="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/blog/D7C19EFE-5872-447A-940E-CC97F45AC25B.jpeg"><link rel="canonical" href="http://gxyrious.github.io/2022/10/25/Amazon-movie-data-processing/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Amazon电影数据处理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-21 13:33:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/blog/D7C19EFE-5872-447A-940E-CC97F45AC25B.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-nz8x1v_1920x1080.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Gxyrious's Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Amazon电影数据处理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-25T04:33:39.000Z" title="发表于 2022-10-25 12:33:39">2022-10-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-01-21T05:33:08.653Z" title="更新于 2023-01-21 13:33:08">2023-01-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%A1%B9%E7%9B%AE/">项目</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h2><p>最近学校有一门课，需要爬取Amazon的电影数据并作清洗和整理，这里简要记录一些思路和困难。</p>
<h2 id="数据爬取"><a href="#数据爬取" class="headerlink" title="数据爬取"></a>数据爬取</h2><ul>
<li><p>准备工作</p>
<p>数据来源是一个包含了7911684个用户评价的txt文档，这里是<a target="_blank" rel="noopener" href="http://snap.stanford.edu/data/web-Movies.html">下载链接</a>。下载完解压后是一个9G左右的movies.txt文档，下面是一条用户评价的示例：</p>
<blockquote>
<p>product&#x2F;productId: B00006HAXW</p>
<p>review&#x2F;userId: A1RSDE90N6RSZF </p>
<p>review&#x2F;profileName: Joseph M. Kotow </p>
<p>review&#x2F;helpfulness: 9&#x2F;9 </p>
<p>review&#x2F;score: 5.0 </p>
<p>review&#x2F;time: 1042502400 </p>
<p>review&#x2F;summary: Pittsburgh - Home of the OLDIES review&#x2F;text: I have all of the doo wop DVD’s and this one is as good or better than the 1st ones. Remember once these performers are gone, we’ll never get to see them again. Rhino did an excellent job and if you like or love doo wop and Rock n Roll you’ll LOVE this DVD !!</p>
</blockquote>
<p>我们会发现，每条评价包含了7个字段，分别是：</p>
<ul>
<li>productId: 产品的asin，可以用来访问该产品的详细网页，如<a target="_blank" rel="noopener" href="http://amazon.com/dp/B00006HAXW/">amazon.com&#x2F;dp&#x2F;B00006HAXW</a></li>
<li>userId: 用户id</li>
<li>profileName: 用户名</li>
<li>helpfulness: fraction of users who found the review helpful</li>
<li>score: rating of the product</li>
<li>time: time of the review (unix time)</li>
<li>summary: review summary</li>
<li>text: text of the review</li>
</ul>
<p>这每个字段都以一行的形式出现在文本文档中，因此只需要用python的<code>readline()</code>函数，再根据字符串的前缀来判断字段属性即可，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;movies.txt&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">movies_list = []</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  line = f.readline()</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      line_gbk = line.decode(<span class="string">&#x27;gbk&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">      <span class="built_in">print</span>(e)</span><br><span class="line">    <span class="comment"># print(line_gbk,end=&quot;&quot;)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;product/productId&#x27;</span> <span class="keyword">in</span> line_gbk:</span><br><span class="line">      asin = line_gbk.split(<span class="string">&#x27;product/productId: &#x27;</span>)[-<span class="number">1</span>].strip(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">      <span class="keyword">if</span> asin <span class="keyword">not</span> <span class="keyword">in</span> movies_list:</span><br><span class="line">        movies_list.append(asin)</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;review/userId&#x27;</span> <span class="keyword">in</span> line_gbk:</span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line">    ......</span><br><span class="line"><span class="built_in">print</span>(movies_list)</span><br></pre></td></tr></table></figure>

<p>这一步，我们需要保存所有电影的asin码到<code>movies_list</code>中，以便在后续爬虫的过程中可以去访问某个产品对应的电影。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">...</span><br><span class="line">np.save(<span class="string">&quot;movies_list.npy&quot;</span>, movies_list)</span><br></pre></td></tr></table></figure>
</li>
<li><p>爬取html源码</p>
<p>由于我有足够的大容量硬盘，我选择的是先爬取html源码，再分析html源码。遍历源码的时间比起重新爬虫而言是很快的，因此在后续若发现有遗漏的电影字段，可以较快地重新分析html源码得到。</p>
<p>我采用selenium库中的webdriver模拟自动化爬虫。首先是可以通过每个产品对应的asin码来访问产品详细页面，即</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">...</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_with_certain_index</span>(<span class="params">start, end, save_path</span>):</span><br><span class="line">  driver = webdriver.Safari()</span><br><span class="line">  <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(start, end + <span class="number">1</span>):</span><br><span class="line">    asin = movies_list[index]</span><br><span class="line">    url = <span class="string">&#x27;https://www.amazon.com/-/en/dp/&#x27;</span> + asin</span><br><span class="line">    driver.get(url)</span><br><span class="line">    source = driver.page_source <span class="comment"># 获取页面源码</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(save_path, asin + <span class="string">&quot;.html&quot;</span>), <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">      f.write(source)</span><br></pre></td></tr></table></figure>

<p>为加快速度，可以采用多线程爬取，在确认线程数时需要注意自己电脑的核数。我用来爬取的电脑是6核的，为了凑整我采用5线程并行爬取。如果超出电脑核数会导致某些线程出错退出，该轮可能就白爬了，因此谨慎谨慎。</p>
<p>对于所有的25w条电影产品，我分0w-10w, 10w-20w, 20w-25w进行爬取。比如在爬取0w-10w时，我分为5个线程，分别爬取0w-2w, 2w-4w, …, 8w-10w，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> read_html <span class="keyword">import</span> read_with_certain_index</span><br><span class="line">save_path = <span class="string">&quot;/Volumes/bGxyrious/DW/WebPages&quot;</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">99999</span></span><br><span class="line">thread_number = <span class="number">5</span> <span class="comment"># 并行爬取的线程数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  total_number = end - start + <span class="number">1</span> <span class="comment"># 爬取总数</span></span><br><span class="line">  single_time = total_number // thread_number <span class="comment"># 每个线程爬取的数目，需要保证能整除</span></span><br><span class="line">  <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(thread_number):</span><br><span class="line">        threading.Thread(</span><br><span class="line">            target=read_with_certain_index,</span><br><span class="line">            args=(start + index * single_time, start + (index + <span class="number">1</span>) * single_time - <span class="number">1</span>, save_path)</span><br><span class="line">        ).start()</span><br></pre></td></tr></table></figure>

<p>使用selenium的自动化爬取，需要安装对应的浏览器驱动，我使用的是Safari和Chrome。关于selenium的使用可以自行学习。</p>
<p>爬取所有html源码用时理论上是30小时左右，由于中途出现几次线程崩溃导致重爬，大约真实采用4-4天才完全爬取完毕，共占据155G。</p>
</li>
<li><p>分析html源码</p>
<p>对于html页面源码的解析可以采用bs4中的BeautifulSoup，当然也可以采用如Xpath等工具。</p>
<p>下面是一个典型并且出现频率最高的页面结构<a target="_blank" rel="noopener" href="http://amazon.com/-/en/dp/B00006HAXW/">B00006HAXW</a>。</p>
<p><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/tj-3-1/product-demo-3.png" alt="product-demo-3"></p>
<p><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/tj-3-1/product-demo-2.png" alt="product-demo-2"></p>
<p>可以发现，主要的信息集中在红色框选中的区域，因此我们对该区域的页面进行解析。比如解析标题，我们查看页面源码，找到标题所在的元素，发现是个span标签：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;productTitle&quot;</span> <span class="attr">class</span>=<span class="string">&quot;a-size-large product-title-word-break&quot;</span>&gt;</span>        Rock Rhythm <span class="symbol">&amp;amp;</span> Doo Wop: Greatest Early Rock       <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>我们用html源码去生成BeautifulSoup对象，然后用该对象的find函数找到这个标题标签，即</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  html_path = <span class="string">&quot;./B00006HAXW.html&quot;</span></span><br><span class="line">  <span class="keyword">with</span> <span class="built_in">open</span>(html_path) <span class="keyword">as</span> file:</span><br><span class="line">    source = file.read()</span><br><span class="line">    soup = BeautifulSoup(source, features=<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    movie_title = get_title(soup)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_title</span>(<span class="params">soup</span>):</span><br><span class="line">  title = <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    span = soup.find(<span class="string">&#x27;span&#x27;</span>, <span class="built_in">id</span>=<span class="string">&quot;productTitle&quot;</span>)</span><br><span class="line">    title = span.text</span><br><span class="line">  <span class="keyword">except</span> Exception <span class="keyword">as</span> error:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">  <span class="keyword">return</span> title</span><br></pre></td></tr></table></figure>

<p>诸如这样，可以获取一系列商品的数据，如标题title、风格genre、发布日期ReleaseDate、演员Actor、导演Director、格式Format、时长Runtime、语言Language等等，将其他保存起来存到csv中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">write_info_to_file</span>(<span class="params">target_list, path</span>):</span><br><span class="line">  <span class="comment"># target_list行数即为所有的商品个数</span></span><br><span class="line">  name = [<span class="string">&quot;asin&quot;</span>, <span class="string">&quot;title&quot;</span>, ...]</span><br><span class="line">  csv_file = pd.DataFrame(columns=name, data=target_list)</span><br><span class="line">  csv_file.to_csv(path)</span><br></pre></td></tr></table></figure>

<p>保存得到的csv文件如下所示：</p>
<p><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/tj-3-1/demo-csv.png" alt="demo-csv"></p>
</li>
</ul>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><blockquote>
<p>对于爬虫得到的源数据，可以发现十分杂乱，还有很多对应的是健身操、音乐歌曲等产品，因此需要对筛选内容、调整格式。</p>
</blockquote>
<h3 id="处理运行时长"><a href="#处理运行时长" class="headerlink" title="处理运行时长"></a>处理运行时长</h3><p>可以发现，运行时长（即上图<em>run_time</em>栏）的格式比较统一，总的而言有以下四种（显然如果只有3秒钟，这肯定不是一部电影，但是在此先不管这件事，本步骤只把运行时长转换为分钟的形式）</p>
<ul>
<li>2 hours and 34 minutes</li>
<li>1 h 35 min</li>
<li>2 hours 或 47 minues</li>
<li>40min 或 3sec</li>
</ul>
<p>这里用了正则表达式和字符串匹配的一些函数，最后将时分秒作转换再返回字符串即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">runtime_cleaner</span>(<span class="params">runtime: <span class="built_in">str</span></span>):</span><br><span class="line">  hour, minute, second = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      runtime_list = runtime.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">      <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(runtime_list)):</span><br><span class="line">        item = runtime_list[index]</span><br><span class="line">        <span class="keyword">if</span> item.isalpha():</span><br><span class="line">          <span class="keyword">if</span> <span class="string">&#x27;hour&#x27;</span> <span class="keyword">in</span> item <span class="keyword">or</span> <span class="string">&#x27;h&#x27;</span> == item: <span class="comment"># &#x27;hour&#x27; in &#x27;hours&#x27;</span></span><br><span class="line">            hour = <span class="built_in">int</span>(runtime_list[index - <span class="number">1</span>])</span><br><span class="line">          <span class="keyword">elif</span> <span class="string">&#x27;minute&#x27;</span> <span class="keyword">in</span> item <span class="keyword">or</span> <span class="string">&#x27;min&#x27;</span> == item: <span class="comment"># &#x27;minute&#x27; in &#x27;minutes&#x27;</span></span><br><span class="line">            minute = <span class="built_in">int</span>(runtime_list[index - <span class="number">1</span>])</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            alphas = re.search(<span class="string">r&#x27;[a-zA-Z]+&#x27;</span>, item)</span><br><span class="line">            numbers = re.search(<span class="string">r&#x27;[0-9]+&#x27;</span>, item)</span><br><span class="line">            <span class="keyword">if</span> alphas <span class="keyword">and</span> numbers:</span><br><span class="line">              alphas = alphas.group()</span><br><span class="line">              numbers = <span class="built_in">int</span>(numbers.group())</span><br><span class="line">              minute = numbers <span class="keyword">if</span> <span class="string">&#x27;min&#x27;</span> <span class="keyword">in</span> alphas <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">              hour = numbers <span class="keyword">if</span> <span class="string">&#x27;h&#x27;</span> <span class="keyword">in</span> alphas <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">              second = numbers <span class="keyword">if</span> <span class="string">&#x27;sec&#x27;</span> <span class="keyword">in</span> alphas <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> error:</span><br><span class="line">      <span class="comment"># print(error)</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">str</span>(hour * <span class="number">60</span> + minute + second // <span class="number">60</span>)</span><br></pre></td></tr></table></figure>

<h3 id="筛选非电影部分"><a href="#筛选非电影部分" class="headerlink" title="筛选非电影部分"></a>筛选非电影部分</h3><p>由于在产品中包括了很多如健身操、电视剧、纪录片、音乐会、短视频等类别，因此需要对非电影部分进行筛选。</p>
<p>首先，最有力的筛选方式是运行时长，这里认为在运行时长存在的情况下（有些产品并不存在运行时长），小于40分钟或大于400分钟的产品，属于非电影。</p>
<p>其次，可以根据关键词进行电影的筛选，关键词主要针对产品的标题title和风格genre，下面是我选择的筛选关键词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">title_keyword = [<span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;Exercise&#x27;</span>, <span class="string">&#x27;PBS&#x27;</span>, <span class="string">&#x27;CD&#x27;</span>, <span class="string">&#x27;Analysis of&#x27;</span>, <span class="string">&#x27;technique&#x27;</span>, <span class="string">&#x27;Collection&#x27;</span>, <span class="string">&#x27;teach&#x27;</span>, <span class="string">&#x27;learn&#x27;</span>, <span class="string">&#x27;instruct&#x27;</span>, <span class="string">&#x27;Hollywood&#x27;</span>, <span class="string">&#x27;Bollywood&#x27;</span>]</span><br><span class="line">genre_keyword = [<span class="string">&#x27;Music Video&#x27;</span>, <span class="string">&#x27;Concert&#x27;</span>, <span class="string">&#x27;Special Interest&#x27;</span>, <span class="string">&#x27;Exercise&#x27;</span>, <span class="string">&#x27;Fitness&#x27;</span>, <span class="string">&#x27;CD&#x27;</span>, <span class="string">&#x27;documentary&#x27;</span>, <span class="string">&#x27;series&#x27;</span>, <span class="string">&#x27;BBC&#x27;</span>, <span class="string">&#x27;episode&#x27;</span>, <span class="string">&#x27;season&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>其中，title关键词中，出现<code>/</code>的一般都是包含了多部电影的产品，这会导致电影的风格、演员、导演等都产生偏差，因此不作记录；对于出现了<code>Exercise, Analysis of, technique, teach, learn, instruct</code>等词的title，为电影的可能性较小，因此也筛除。</p>
<p>在genre关键词中，<code>Music Video, Concert, CD</code>表示音乐相关，<code>Exercise, Fitness</code>表示运动健身相关，<code>documentary</code>表示纪录片、<code>series, episode, season</code>表示连续剧，<code>BBC</code>表示广播视频。以上这些都是非电影，因而直接筛除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isMovie</span>(<span class="params">row: pd.Series</span>):</span><br><span class="line">    run_time = <span class="built_in">int</span>(row[<span class="string">&quot;run_time&quot;</span>])</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pd.isna(run_time) <span class="keyword">and</span> run_time != <span class="number">0</span> <span class="keyword">and</span> (run_time &gt;= <span class="number">400</span> <span class="keyword">or</span> run_time &lt;= <span class="number">40</span>):</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    title, genre = row[<span class="string">&#x27;title&#x27;</span>], row[<span class="string">&#x27;genre&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> kw <span class="keyword">in</span> title_keyword:</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> pd.isna(title) <span class="keyword">and</span> kw <span class="keyword">in</span> title:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> kw <span class="keyword">in</span> genre_keyword:</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> pd.isna(title) <span class="keyword">and</span> kw <span class="keyword">in</span> genre:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">movies_filter</span>(<span class="params">movies_info: pd.DataFrame</span>):</span><br><span class="line">    waiting_delete_index = []</span><br><span class="line">    <span class="keyword">for</span> index, row <span class="keyword">in</span> movies_info.iterrows():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isMovie(row):</span><br><span class="line">            waiting_delete_index.append(index)</span><br><span class="line">    <span class="keyword">return</span> movies_info.drop(index=waiting_delete_index)</span><br></pre></td></tr></table></figure>

<h3 id="处理电影标题"><a href="#处理电影标题" class="headerlink" title="处理电影标题"></a>处理电影标题</h3><p>通过观察数据，我发现电影标题中存在描述产品信息的<code>()[]</code>，比如<code>Sideshow:Alive on  the Inside [VHS]</code>或<code>A Passage To India  (2-Disc Collectors Edition)</code>，它们是冗余的电影标题信息，利用正则表达式可以很容易地将字符串中的括号匹配出来并删除；此外，电影标题中还存在引号不规范、括号不匹配、换行符多余的情况，在这里都一并使用正则表达式处理；最后将标题前后的括号进行去除，代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">title_cleaner</span>(<span class="params">target: pd.Series</span>):</span><br><span class="line">    <span class="comment"># 清理标题</span></span><br><span class="line">    target = remove_brackets(target)</span><br><span class="line">    target = remove_special_characters(target)</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_special_characters</span>(<span class="params">target: pd.Series</span>):</span><br><span class="line">    <span class="comment"># 删除替换不规范字符</span></span><br><span class="line">    target = target.apply(<span class="keyword">lambda</span> c: c.encode(<span class="string">&#x27;utf-8&#x27;</span>).decode())</span><br><span class="line">    target = target.apply(</span><br><span class="line">        <span class="keyword">lambda</span> c: re.sub(<span class="string">r&quot;‘|’|”|“&quot;</span>, <span class="string">&quot;&#x27;&quot;</span>, c).strip()</span><br><span class="line">    )</span><br><span class="line">    target = target.apply(</span><br><span class="line">        <span class="keyword">lambda</span> c: re.sub(<span class="string">r&quot;\n|\r|&#x27;\n|&#x27;\r&quot;</span>, <span class="string">&quot;&quot;</span>, c).strip()</span><br><span class="line">    )</span><br><span class="line">    target = target.apply(</span><br><span class="line">        <span class="keyword">lambda</span> c: c.strip()</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> target</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_brackets</span>(<span class="params">target: pd.Series</span>):</span><br><span class="line">    <span class="comment"># 删除括号</span></span><br><span class="line">    target = target.apply(</span><br><span class="line">        <span class="keyword">lambda</span> c: re.sub(<span class="string">r&quot;\(.*?\)|\&#123;.*?&#125;|\[.*?]&quot;</span>, <span class="string">&quot;&quot;</span> , c)</span><br><span class="line">        )</span><br><span class="line">    target = target.apply(</span><br><span class="line">        <span class="keyword">lambda</span> c: re.sub(<span class="string">r&quot;\(.*?$|\&#123;.*?$|\[.*?$&quot;</span>, <span class="string">&quot;&quot;</span> , c)</span><br><span class="line">        )</span><br><span class="line">    target = target.apply(</span><br><span class="line">        <span class="keyword">lambda</span> c: re.sub(<span class="string">r&quot;\)|\]|\&#125;&quot;</span>, <span class="string">&quot;&quot;</span>, c)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> target</span><br></pre></td></tr></table></figure>

<h3 id="处理各个字段信息"><a href="#处理各个字段信息" class="headerlink" title="处理各个字段信息"></a>处理各个字段信息</h3><ul>
<li><p>language</p>
<p>language和标题字段类似，都需要删除括号和特殊字符，此外还有一些产品的language字段为空，在csv中会自动赋值为nan，在这里统一填充为空字符，以方便后续处理。</p>
</li>
<li><p>genre, actor, director, format, language, release_date, first_available_date, run_time</p>
<p>这些字符串不需要额外的处理，只需要将一些特殊字符去除即可，因此调用<code>remove_special_characters</code>函数即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">value_cleaner</span>(<span class="params">movies_info: pd.DataFrame</span>):</span><br><span class="line">    movies_info.fillna(value=<span class="string">&#x27;&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> attr <span class="keyword">in</span> (<span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;genre&#x27;</span>, <span class="string">&#x27;actor&#x27;</span>, <span class="string">&#x27;director&#x27;</span>, <span class="string">&#x27;format&#x27;</span>, <span class="string">&#x27;language&#x27;</span>, <span class="string">&#x27;release_date&#x27;</span>, <span class="string">&#x27;first_available_date&#x27;</span>, <span class="string">&#x27;run_time&#x27;</span>):</span><br><span class="line">        movies_info[attr] = remove_special_characters(movies_info[attr])</span><br><span class="line">    <span class="keyword">return</span> movies_info</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="转换为List列表"><a href="#转换为List列表" class="headerlink" title="转换为List列表"></a>转换为List列表</h3><p>由于很多字段都由多个单值组成，如actor中由很多个演员名称组成，因此在这里将这些字段转换为List以方便后续的处理。</p>
<p>各个单值都是使用逗号进行分割的，因此直接使用<code>.split(&#39;,&#39;)</code>转换为List，再使用<code>.stript()</code>去掉前后的空格即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> attr <span class="keyword">in</span> (<span class="string">&#x27;genre&#x27;</span>, <span class="string">&#x27;actor&#x27;</span>, <span class="string">&#x27;director&#x27;</span>, <span class="string">&#x27;format&#x27;</span>, <span class="string">&#x27;language&#x27;</span>):</span><br><span class="line">    movies_info[attr] = movies_info[attr].apply(convert_to_list)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_to_list</span>(<span class="params">list_str: <span class="built_in">str</span></span>):</span><br><span class="line">    new_list = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> list_str: list_str.strip(), list_str.split(<span class="string">&#x27;,&#x27;</span>)))</span><br><span class="line">    <span class="keyword">while</span> <span class="string">&#x27;&#x27;</span> <span class="keyword">in</span> new_list:</span><br><span class="line">        new_list.remove(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> new_list</span><br></pre></td></tr></table></figure>

<h3 id="日期标准化"><a href="#日期标准化" class="headerlink" title="日期标准化"></a>日期标准化</h3><p>对于数据中的日期，总体而言有三种形式，空缺、纯数字年份和年月日日期。对于空缺值仍然保留，对于纯数字年份填充为2022-0-0，对于完整的日期则化简为2022-1-1的形式。</p>
<p>年月日日期的格式都形如<code>August 10, 2010</code>的格式，因此先设置本机地域，然后采用时间类<code>datetime.striptime</code>去格式化解析时间，并转化为标准形态<code>2022-1-1</code>。</p>
<p>对于空缺值，在电影合并的时候会从第三方数据源重新获取，会在后面进行说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> attr <span class="keyword">in</span> (<span class="string">&#x27;release_date&#x27;</span>, <span class="string">&#x27;first_available_date&#x27;</span>):</span><br><span class="line">    movies_info[attr] = movies_info[attr].apply(date_cleaner)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">date_cleaner</span>(<span class="params">date_str: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="keyword">if</span> date_str == <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">elif</span> date_str.isdigit():</span><br><span class="line">        date_str = <span class="string">&#x27;&#123;&#125;-00-00&#x27;</span>.<span class="built_in">format</span>(date_str)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            locale.setlocale(locale.LC_ALL, <span class="string">&#x27;en_US.utf8&#x27;</span>)</span><br><span class="line">            date_str = <span class="built_in">str</span>(datetime.strptime(date_str, <span class="string">&#x27;%B %d, %Y&#x27;</span>).date())</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> error:</span><br><span class="line">            <span class="built_in">print</span>(error)</span><br><span class="line">    <span class="keyword">return</span> date_str</span><br></pre></td></tr></table></figure>

<h3 id="合并电影"><a href="#合并电影" class="headerlink" title="合并电影"></a>合并电影</h3><p>由于爬虫的数据源是电影的各种产品，而不同产品可能对应同一部电影，因此需要对相同的电影进行合并。这里采用的合并策略主要依据电影名，由于已经对电影title进行清理，同一部电影的名称有高度的相似性，因此该方法是合理的。在后续查看合并日志的时候，我发现合并的电影确实都是名字相同或高度类似的。</p>
<ul>
<li><p>模糊匹配</p>
<p>电影名的匹配主要采用<code>rapidfuzz.process</code>模块中的<code>.extractOne()</code>方法，采用的字符串相似度计算方法是<strong>Levenshtein Distance</strong>算法，是指两个字符串之间，由一个转成另一个所需的最少编辑操作次数。许可的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。一般来说，编辑距离越小，两个串的相似度越高。</p>
<p>该方法的参数是字符串和字符串List，返回该字符串在字符串List中的最高匹配对象以及相似度评分。经过数据的预测试，我发现将评分为95以上的电影名基本是同一部电影，它们相差的可能只是一些空格、标点符号，因此可以认为是同一部电影。</p>
</li>
<li><p>合并策略</p>
<p>先设置一个空的影名字典<code>title_dict</code>（此处为了存储电影名在本来的DataFrame中的索引，将列表替换为字典）和空的待删除索引列表<code>waiting_delete_list</code>，然后遍历所有电影名，使用上述模糊匹配算法，计算某个电影名在影名列表中的匹配程度，由于它是第一次遍历到，因此命名为<code>new_name</code>，而本来就在影名列表中存在的，命名为<code>old_name</code>。</p>
<p>如果评分超过95，说明在影名列表中已经有相同电影，此时将这两部电影的信息进行比对整合，并将new_name的索引添加到待删除索引列表，将并更新血缘关系表；如果评分没有超过95，说明该电影未出现，则将其加入电影列表，并更新血缘关系表。</p>
</li>
<li><p>字段的合并</p>
<p>字段的合并策略分为三类，下面分别进行讲述：</p>
<ol>
<li><p>影名<code>title</code></p>
<p>采取更长影名作为新的影名</p>
</li>
<li><p>列表值<code>genre, actor, director, format, language</code> </p>
<p>将新值在旧值列表中进行相似度匹配，如果评分低于95就添加为新值，否则直接废弃；空缺值也包含于上述的处理方法之中。</p>
</li>
<li><p>日期值</p>
<p>由于日期值都很多空缺，因此采用第三方数据源进行搜索。我维持每部电影的日期值不空缺，即当出现新值时，若是空缺，则前往第三方数据源进行查找。当出现影名列表中已有的电影时，若日期的新值不为空，则比较两者并选出更早的日期值。</p>
</li>
<li><p>运行时长</p>
<p>类似影名，选择运行时间更长的，并更新血缘关系表。</p>
</li>
</ol>
</li>
</ul>
<h3 id="第三方数据源"><a href="#第三方数据源" class="headerlink" title="第三方数据源"></a>第三方数据源</h3><p>当有相关数据在亚马逊无法爬取获得时，我们需要寻找第三方数据源进行补充，这里我主要采用来自imdb作为数据源的补充。</p>
<p>在<a target="_blank" rel="noopener" href="https://imdb-api.com/swagger/index.html">https://imdb-api.com/swagger/index.html</a>中提供了对IMDB数据的查找API，在注册账号后就可以免费获取apiKey，通过该apiKey便可以调用其提供的api。我主要采用了两个api，如下所示：（我的apiKey&#x3D;k_fjhihha0）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> pd.isna(row[<span class="string">&#x27;release_date&#x27;</span>]):</span><br><span class="line">    url1 = <span class="string">&#x27;https://imdb-api.com/API/SearchMovie/k_fjhihha0/&#x27;</span> + title</span><br><span class="line">    response = requests.get(url1) <span class="comment"># 第一个</span></span><br><span class="line">    res_json = json.loads(response.text)</span><br><span class="line">    movie_id = res_json.get(<span class="string">&#x27;results&#x27;</span>)[<span class="number">0</span>].get(<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">    url2 = <span class="string">&#x27;https://imdb-api.com/en/API/Title/k_fjhihha0/&#x27;</span> + movie_id</span><br><span class="line">    response = requests.get(url2) <span class="comment"># 第二个</span></span><br><span class="line">    movie_json = json.loads(response.text)</span><br><span class="line">    release_date = movie_json.get(<span class="string">&#x27;releaseDate&#x27;</span>)</span><br><span class="line">    new_series_dict[<span class="string">&#x27;release_date&#x27;</span>] = url2</span><br><span class="line">    movies_info.loc[index, <span class="string">&#x27;release_date&#x27;</span>] = release_date</span><br></pre></td></tr></table></figure>

<ol>
<li>第一个api是通过电影名查找匹配的电影，并获取这些电影的标识符id等信息，组成列表。若某些信息在亚马逊不存在，则使用影名去调用该api，以获取电影标识符，再进一步通过下面的api获取电影信息。</li>
<li>第二个api是通过电影标识符id查找该电影的详细信息，上述代码中，我获取了放映日期这个信息，作为原来数据源的补充。</li>
</ol>
<h3 id="处理相同姓名（演员、导演）"><a href="#处理相同姓名（演员、导演）" class="headerlink" title="处理相同姓名（演员、导演）"></a>处理相同姓名（演员、导演）</h3><p>相同姓名的处理类似合并电影名，设置一个空的姓名列表，并遍历所有的演员、导演名，由于导演和演员之间可能存在交叉，因此两者共用一个姓名列表。对于某一个姓名，查询它在姓名列表中的匹配率，如果评分大于95分，则将该名称进行替换；否则就将其加入新的姓名列表。并且可以新建一个姓名表并设置主键索引，将主表中的演员、导演字段替换为姓名表的外键，从而减少主表查询的负担。</p>
<h2 id="处理结果"><a href="#处理结果" class="headerlink" title="处理结果"></a>处理结果</h2><ol>
<li><p>电影信息表</p>
<p>在电影的评论中，一共有25w+条数据，在爬虫后剔除了一些失效页面后，还剩下24w+条数据，经过电影产品的筛选、相同电影的合并，最终剩下10w+电影产品。</p>
</li>
</ol>
<p><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/tj-3-1/Screen%20Shot%202022-10-30%20at%2022.43.48.png" alt="Screen Shot 2022-10-30 at 22.43.48"></p>
<p>相比其他同学的结果，我最终剩余的数量偏少，在查看了合并日志(merge_log.txt)后，即如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">Triumph of Love ==? Triumph of Love</span><br><span class="line">Benji ==? Benji</span><br><span class="line">Stargate ==? Stargate</span><br><span class="line">Mystery Men ==? Mystery Men</span><br><span class="line">Circus ==? Circus</span><br><span class="line">Enchanted April ==? Enchanted April</span><br><span class="line">My Little Chickadee ==? My Little Chickadee</span><br><span class="line">Intermission ==? Intermission</span><br><span class="line">The Comedy Jesus Show ==? The Comedy Jesus Show</span><br><span class="line">Open Water ==? Open Water</span><br><span class="line">Secondhand Lions ==? Secondhand Lions</span><br><span class="line">Fearless ==? Fearless</span><br><span class="line">The Pebble and the Penguin ==? The Pebble and the Penguin</span><br><span class="line">Gunga Din ==? Gunga Din</span><br><span class="line">Limelight ==? Limelight</span><br><span class="line">Age of Heroes ==? Age of Heroes</span><br><span class="line">Aliens ==? Aliens</span><br><span class="line">Year of Getting to Know Us ==? Year of Getting to Know Us</span><br><span class="line">The Cops are Robbers ==? The Cops Are Robbers</span><br><span class="line">Problem Child ==? Problem Child</span><br><span class="line">A Marine Story ==? A Marine Story</span><br><span class="line">Live Intrusion VHS ==? Live Intrusion VHS</span><br><span class="line">David E.David E. Talbert&#x27;s A Fool and His Money ==? David E. Talbert&#x27;s A Fool and His Money</span><br><span class="line">Gitana Tenias Que Ser ==? Gitana Tenias Que Ser</span><br><span class="line">Hangar 18Hangar 18 ==? Hangar 18</span><br><span class="line">Integrated Qi Gong for Lower Back Pain ==? Integrated Qi Gong for lower back pain</span><br><span class="line">77 Angels in Eden ==? 7 ANGELS IN EDEN</span><br><span class="line">The Sitter ==? The Sitter</span><br><span class="line">Monsters of Metal: The Ultimate Metal Compilation, Vol. 1Monsters of Metal: The Ultimate Metal Compilation, Vol. 1 ==? Monsters of Metal: The Ultimate Metal Compilation, Vol. 2</span><br><span class="line">Sex &amp; Zen :Sex &amp; Zen : Extreme Ecstasy ==? Sex &amp; Zen: Extreme Ecstasy</span><br><span class="line">Por Ellas Aunque Mal Paguen ==? Por Ellas Aunque Mal Paguen</span><br><span class="line">Mad Dog Morgan ==? Mad Dog Morgan</span><br><span class="line">The Escapist ==? The Escapist</span><br><span class="line">Sitting Ducks ==? Sitting Ducks</span><br><span class="line">Baby on Board ==? Baby On Board</span><br><span class="line">Mr.Mr. Winkle Goes to War ==? Mr. Winkle Goes To War</span><br><span class="line">Bad Seed ==? Bad Seed</span><br><span class="line">True Blue ==? True Blue</span><br><span class="line">Wreckage ==? Wreckage</span><br><span class="line">The Unit ==? The Unit</span><br><span class="line">Jaded ==? Jaded</span><br><span class="line">The Dresser ==? The Dresser</span><br><span class="line">Seventh Brother ==? Seventh Brother</span><br><span class="line">The Stationmasters Wife ==? The Stationmasters Wife</span><br><span class="line">Beneath the Planet of the Apes ==? Beneath the Planet of the Apes</span><br><span class="line">Ransom ==? Ransom</span><br><span class="line">The Land Before Time -The Land Before Time - The Big Freeze ==? The Land Before Time - The Big Freeze</span><br><span class="line">Jackson Pollock ==? Jackson Pollock</span><br><span class="line">Interview with the Vampire:Interview with the Vampire: The Vampire Chronicles ==? Interview with the Vampire: The Vampire Chronicles</span><br><span class="line">Hatchet For A Honeymoon ==? Hatchet for a Honeymoon</span><br><span class="line">Space Marines ==? Space Marines</span><br><span class="line">Strauss: The King of Three-Strauss: The King of Three-Quarter Time ==? Strauss: The King of Three-Quarter Time</span><br><span class="line">Just Ask My Children ==? Just Ask My Children</span><br><span class="line">Bluebeard ==? Bluebeard</span><br><span class="line">Virasat ==? Virasat</span><br><span class="line">Tai Chi for Diabetes ==? Tai Chi for Diabetes</span><br><span class="line">Fever ==? Fever</span><br><span class="line">Four Horsemen of the Apocalypse ==? Four Horsemen of the Apocalypse</span><br><span class="line">Little Man ==? Little Man</span><br><span class="line">Ship Ahoy ==? Ship Ahoy</span><br></pre></td></tr></table></figure>

<p>我发现大多数电影的合并确实是有理有据的，因此思考可能是非电影产品的筛选过于严格，在经过相关参数的调整后重新运行，电影的总数达到了12w。</p>
<ol start="2">
<li><p>血缘关系表</p>
<p>在合并的过程中，记录了每个字段的来源，如亚马逊、豆瓣、IMDB等。如果有多个来源，则将其用逗号隔开。这张表的主键为电影名。通过电影名的合并，每个影名都是唯一的标识符，考虑到查询的速度，后续会将其用数字id代替。</p>
</li>
</ol>
<p><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/tj-3-1/Screen%20Shot%202022-10-30%20at%2022.59.20.png" alt="Screen Shot 2022-10-30 at 22.59.20"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://gxyrious.github.io">Gxyrious</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://gxyrious.github.io/2022/10/25/Amazon-movie-data-processing/">http://gxyrious.github.io/2022/10/25/Amazon-movie-data-processing/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://gxyrious.github.io" target="_blank">Gxyrious's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a></div><div class="post_share"><div class="social-share" data-image="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-nz8x1v_1920x1080.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/11/01/basic-configuration-for-vue/"><img class="prev-cover" src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-k9dmx7_1920x1080.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Vue基础配置</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/26/usage-of-apple-object-capture/"><img class="next-cover" src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-4yqkxn_1920x1080.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">苹果Object-Capture使用</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/blog/D7C19EFE-5872-447A-940E-CC97F45AC25B.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gxyrious</div><div class="author-info__description">你看 月亮正对着你笑</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Gxyrious"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Gxyrious" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:chang.liu.me@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">日常瞎写，自我记录，不定更新</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">序言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-number">2.</span> <span class="toc-text">数据爬取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="toc-number">3.</span> <span class="toc-text">数据清洗</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E8%BF%90%E8%A1%8C%E6%97%B6%E9%95%BF"><span class="toc-number">3.1.</span> <span class="toc-text">处理运行时长</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%9B%E9%80%89%E9%9D%9E%E7%94%B5%E5%BD%B1%E9%83%A8%E5%88%86"><span class="toc-number">3.2.</span> <span class="toc-text">筛选非电影部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%94%B5%E5%BD%B1%E6%A0%87%E9%A2%98"><span class="toc-number">3.3.</span> <span class="toc-text">处理电影标题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E5%90%84%E4%B8%AA%E5%AD%97%E6%AE%B5%E4%BF%A1%E6%81%AF"><span class="toc-number">3.4.</span> <span class="toc-text">处理各个字段信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E4%B8%BAList%E5%88%97%E8%A1%A8"><span class="toc-number">3.5.</span> <span class="toc-text">转换为List列表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E6%9C%9F%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">3.6.</span> <span class="toc-text">日期标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E7%94%B5%E5%BD%B1"><span class="toc-number">3.7.</span> <span class="toc-text">合并电影</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%96%B9%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">3.8.</span> <span class="toc-text">第三方数据源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%9B%B8%E5%90%8C%E5%A7%93%E5%90%8D%EF%BC%88%E6%BC%94%E5%91%98%E3%80%81%E5%AF%BC%E6%BC%94%EF%BC%89"><span class="toc-number">3.9.</span> <span class="toc-text">处理相同姓名（演员、导演）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%9C"><span class="toc-number">4.</span> <span class="toc-text">处理结果</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/10/13/hadoop-configuration-with-docker-simulating-distributed-nodes/" title="Docker模拟分布式结点配置Hadoop"><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-4ovp87_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker模拟分布式结点配置Hadoop"/></a><div class="content"><a class="title" href="/2024/10/13/hadoop-configuration-with-docker-simulating-distributed-nodes/" title="Docker模拟分布式结点配置Hadoop">Docker模拟分布式结点配置Hadoop</a><time datetime="2024-10-13T11:46:43.000Z" title="发表于 2024-10-13 19:46:43">2024-10-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/23/installation-of-mysql-in-ubuntu-with-docker/" title="Ubuntu使用Docker安装Mysql"><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-4lzz6p_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ubuntu使用Docker安装Mysql"/></a><div class="content"><a class="title" href="/2024/09/23/installation-of-mysql-in-ubuntu-with-docker/" title="Ubuntu使用Docker安装Mysql">Ubuntu使用Docker安装Mysql</a><time datetime="2024-09-23T14:02:40.000Z" title="发表于 2024-09-23 22:02:40">2024-09-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/26/macos-external-hard-drive-install-docker/" title="MacOS在外接硬盘中安装docker"><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-k9dmx7_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MacOS在外接硬盘中安装docker"/></a><div class="content"><a class="title" href="/2024/08/26/macos-external-hard-drive-install-docker/" title="MacOS在外接硬盘中安装docker">MacOS在外接硬盘中安装docker</a><time datetime="2024-08-26T08:11:22.000Z" title="发表于 2024-08-26 16:11:22">2024-08-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/17/spring-rain-Suichuan-Jinshi-Summer-Camp/" title="春雨遂川润泽支教夏令营"><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/blog/spring-rain-Suichuan-Jinshi-Summer-Camp/tongji-spring-rain.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="春雨遂川润泽支教夏令营"/></a><div class="content"><a class="title" href="/2024/08/17/spring-rain-Suichuan-Jinshi-Summer-Camp/" title="春雨遂川润泽支教夏令营">春雨遂川润泽支教夏令营</a><time datetime="2024-08-17T06:33:08.000Z" title="发表于 2024-08-17 14:33:08">2024-08-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/13/xinjiang-journey-northern-loop/" title="新疆游记——北疆大环线"><img src="https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/blog/xinjiang-journey-northern-loop/image-20240713235720028.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="新疆游记——北疆大环线"/></a><div class="content"><a class="title" href="/2024/07/13/xinjiang-journey-northern-loop/" title="新疆游记——北疆大环线">新疆游记——北疆大环线</a><time datetime="2024-07-13T15:31:56.000Z" title="发表于 2024-07-13 23:31:56">2024-07-13</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://gxyrious.oss-cn-hangzhou.aliyuncs.com/img/wallpapers/wallhaven-nz8x1v_1920x1080.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Gxyrious</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script></div></body></html>